{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda  <a name=\"backtotop\"></a>\n",
    "---\n",
    "\n",
    "1. [One-Layer Neural Network](#OneLayerNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder RNN\n",
    "\n",
    "[back to top](#backtotop) <br>\n",
    "**Applications:**\n",
    "- Machine Translation\n",
    "- Text Summarization\n",
    "- Related Search/Query Suggestion (used in linkedin to find similar job)\n",
    "- Video description\n",
    "\n",
    "\n",
    "**Training Data in Translation Project:**\n",
    "- Reformulated queries from session logs\n",
    "- At least one word in common\n",
    "- Remove relaxed query (\"a, b, c\" -> \"a, c) pairs to reduce bias in model output\n",
    "- Do not unique repeated pairs\n",
    "\n",
    "***\n",
    "The Following shows the application of the Encoder-Decoder RNN in language translation. We can see **English words are encoded into a single matrix by the left RNN and then decoded to French by another RNN**. In this case the single matrix is simply h7 (last matrix from the left RNN) \n",
    "\n",
    "<img src = \"img/Encoder-Decoder-Model.png\"></img>\n",
    "***\n",
    "\n",
    "In the French decoder, we could choose max prosibility from softmax as our final result. Otherwise, we use Beam Search to Keep the top k number of posibility from the last epoch and create a tree-like graph to obtain multiple possible result. At the end, we use the one with the highest score. The prevent mistakes in the process on certain words\n",
    "<img src = \"img/Beam serach in RNN.png\"></img>\n",
    "***\n",
    "\n",
    "However, there is a problem to the model. **When we are predicting lait in French(Means milk), it does not really think about cat**\n",
    "<img src = \"img/Encoder-Decoder-Model-Problem.png\"></img>\n",
    "\n",
    "Fortunately, the problem can be solved by adding attention model to it. In attention model, we compare the output from each epoch at the right side with every vocab at the left RNN. We could use Dot product, multiplicative or just additive\n",
    "<img src = \"img/Encoder-Decoder-Model-Attention.png\"></img>\n",
    "***\n",
    "\n",
    "\n",
    "*Reference: https://www.youtube.com/watch?v=bBBYPuVUnug*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Base - Support Vector Machine (NB-SVM)\n",
    "- Naive Base(NB) and Support Vector Machine(SVM) are widely used as base-line in test-related task. Their accuracy varies significantly across varients, features and datasets\n",
    "- **NB does better job than SVM in short snippet sentiment tasks, while SVM outperforms NB in long documents**\n",
    "- A SVM varients using NB log-count ratio as a feature constantly performs well\n",
    "\n",
    "*Reference: https://sijunhe.github.io/blog/2018/04/03/nb-svm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Preceptron Neural Network\n",
    "\n",
    "- There is a theoretical finding by Lippmann in the 1987 paper \"An introduction to computing with neural networkds\" that shows that **a MLP with two hidden layer is sufficient to represent any shape in classification problems.**\n",
    "- Although there are some functions can be approximated by one large hidden layer, it can be learned more efficiently by two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
