{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This model aim to try around the accuracy over <br>\n",
    "1. **Logistic Regression**\n",
    "2. **Multinomial Naive Bayes**\n",
    "3. **Random Forest**\n",
    "4. **Gradient Boosting Machines**\n",
    "5. **Naive Bayes SVM**\n",
    "6. **Multilayer Perceptron Neural Network(MLP)**\n",
    "7. **LSTM Neural Network**\n",
    "8. **Bidirectional LSTM Neural Network**\n",
    "9. **Convolutional Neural Network**\n",
    "\n",
    "Data type <br>\n",
    "- **x: a tfidf matrix derived from news headline data from Reddit WorldNews Channel**\n",
    "- **y: binary classification derived from rising/dropping signal of DJIA**\n",
    "\n",
    "Return value: a sorted dataframe recording accuracy from the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import six\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.utils import check_X_y, check_array\n",
    "from sklearn.preprocessing import LabelBinarizer, normalize\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from scipy.sparse import issparse\n",
    "from scipy import sparse\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from tqdm import tqdm\n",
    "from abc import ABCMeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "DJIA_fn = \"DJIA_table.csv\"\n",
    "News_fn = \"Combined_News_DJIA.csv\"\n",
    "\n",
    "DJIA_df = pd.read_csv(path + DJIA_fn)\n",
    "DJIA_df = DJIA_df.sort_values(\"Date\")\n",
    "DJIA_df.index = range(len(DJIA_df))\n",
    "\n",
    "News_df = pd.read_csv(path + News_fn)\n",
    "News_df = News_df.sort_values(\"Date\")\n",
    "News_df.index = range(len(News_df))\n",
    "\n",
    "\n",
    "def RemoveQuote(tmp_str):\n",
    "    s_quote = False\n",
    "    d_quote = False\n",
    "    start_list = []\n",
    "    end_list = []\n",
    "    mid_str = tmp_str[5:len(tmp_str)-5]\n",
    "    for i in range(5):\n",
    "        if (not s_quote) or (not d_quote):\n",
    "            try:\n",
    "                if tmp_str[i] != \"\\'\" and tmp_str[i] != \"\\\"\":\n",
    "                    if i <=2 and tmp_str[i] == 'b':\n",
    "                        continue\n",
    "                    start_list.append(tmp_str[i])\n",
    "                if tmp_str[-5+i] != \"'\" and tmp_str[-5+i] != '\"':\n",
    "                    end_list.append(tmp_str[-5+i])\n",
    "            except:\n",
    "                print(tmp_str)\n",
    "\n",
    "    tmp_str = \"\".join(start_list) + mid_str + \"\".join(end_list)\n",
    "    return tmp_str\n",
    "\n",
    "headline_columns = [x for x in News_df.columns if re.match(\"Top\", x)]\n",
    "for col in headline_columns:\n",
    "    News_df[col] = News_df[col].apply(lambda x: RemoveQuote(x) if x == x else x)\n",
    "Comb_df = DJIA_df.merge(News_df, on = \"Date\", how = \"inner\")\n",
    "\n",
    "\n",
    "train_index = pd.to_datetime(DJIA_df.Date, format = \"%Y-%m-%d\") < pd.to_datetime(\"2014-12-31\", format = \"%Y-%m-%d\")\n",
    "train_data = Comb_df[train_index]\n",
    "test_data = Comb_df[~train_index]\n",
    "joint_headlines_train = train_data[headline_columns[0]]\n",
    "joint_headlines_test = test_data[headline_columns[0]]\n",
    "\n",
    "for i in range(1, len(headline_columns)):\n",
    "    joint_headlines_train += (' ' + train_data[headline_columns[i]].apply(lambda x: str(x) if x == x else \"\"))\n",
    "    joint_headlines_test += (' ' + test_data[headline_columns[i]].apply(lambda x: str(x) if x == x else \"\"))\n",
    "\n",
    "basicVetorizer = TfidfVectorizer(min_df=0.03, max_df=0.97, max_features = 200000, ngram_range = (2, 2))\n",
    "basic_train = basicVetorizer.fit_transform([x for x in joint_headlines_train.values if x == x])\n",
    "basic_test = basicVetorizer.transform([x for x in joint_headlines_test.values if x == x])\n",
    "basic_whole = basicVetorizer.fit_transform([x for x in joint_headlines_train.append(joint_headlines_test).values if x==x])\n",
    "\n",
    "X_train = basic_train\n",
    "X_test = basic_test\n",
    "\n",
    "Y_train = train_data.Label.values\n",
    "Y_test = test_data.Label.values\n",
    "\n",
    "X_raw_text_train = joint_headlines_train\n",
    "X_raw_text_test = joint_headlines_test\n",
    "\n",
    "X = basic_whole\n",
    "Y = Comb_df.Label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NBSVM(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n",
    "\n",
    "    def __init__(self, alpha=1.0, C=1.0, max_iter=10000):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.svm_ = [] # fuggly\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y, 'csr')\n",
    "        _, n_features = X.shape\n",
    "\n",
    "        labelbin = LabelBinarizer()\n",
    "        Y = labelbin.fit_transform(y)\n",
    "        self.classes_ = labelbin.classes_\n",
    "        if Y.shape[1] == 1:\n",
    "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
    "\n",
    "        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n",
    "        # so we don't have to cast X to floating point\n",
    "        Y = Y.astype(np.float64)\n",
    "\n",
    "        # Count raw events from data\n",
    "        n_effective_classes = Y.shape[1]\n",
    "        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
    "        self.ratios_ = np.full((n_effective_classes, n_features), self.alpha,\n",
    "                                 dtype=np.float64)\n",
    "        self._compute_ratios(X, Y)\n",
    "\n",
    "        # flugglyness\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            svm = LinearSVC(C=self.C, max_iter=self.max_iter)\n",
    "            Y_i = Y[:,i]\n",
    "            svm.fit(X_i, Y_i)\n",
    "            self.svm_.append(svm) \n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_effective_classes = self.class_count_.shape[0]\n",
    "        n_examples = X.shape[0]\n",
    "\n",
    "        D = np.zeros((n_effective_classes, n_examples))\n",
    "\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            D[i] = self.svm_[i].decision_function(X_i)\n",
    "        \n",
    "        return self.classes_[np.argmax(D, axis=0)]\n",
    "        \n",
    "    def _compute_ratios(self, X, Y):\n",
    "        \"\"\"Count feature occurrences and compute ratios.\"\"\"\n",
    "        if np.any((X.data if issparse(X) else X) < 0):\n",
    "            raise ValueError(\"Input X must be non-negative\")\n",
    "\n",
    "        self.ratios_ += safe_sparse_dot(Y.T, X)  # ratio + feature_occurrance_c\n",
    "        normalize(self.ratios_, norm='l1', axis=1, copy=False)\n",
    "        row_calc = lambda r: np.log(np.divide(r, (1 - r)))\n",
    "        self.ratios_ = np.apply_along_axis(row_calc, axis=1, arr=self.ratios_)\n",
    "        check_array(self.ratios_)\n",
    "        self.ratios_ = sparse.csr_matrix(self.ratios_)\n",
    "\n",
    "        #p_c /= np.linalg.norm(p_c, ord=1)\n",
    "        #ratios[c] = np.log(p_c / (1 - p_c))\n",
    "\n",
    "\n",
    "def f1_class(pred, truth, class_val):\n",
    "    n = len(truth)\n",
    "\n",
    "    truth_class = 0\n",
    "    pred_class = 0\n",
    "    tp = 0\n",
    "\n",
    "    for ii in range(0, n):\n",
    "        if truth[ii] == class_val:\n",
    "            truth_class += 1\n",
    "            if truth[ii] == pred[ii]:\n",
    "                tp += 1\n",
    "                pred_class += 1\n",
    "                continue;\n",
    "        if pred[ii] == class_val:\n",
    "            pred_class += 1\n",
    "\n",
    "    precision = tp / float(pred_class)\n",
    "    recall = tp / float(truth_class)\n",
    "\n",
    "    return (2.0 * precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def semeval_senti_f1(pred, truth, pos=2, neg=0): \n",
    "\n",
    "    f1_pos = f1_class(pred, truth, pos)\n",
    "    f1_neg = f1_class(pred, truth, neg)\n",
    "\n",
    "    return (f1_pos + f1_neg) / 2.0;\n",
    "\n",
    "\n",
    "def main(train_file, test_file, ngram=(1, 3)):\n",
    "    print('loading...')\n",
    "    train = pd.read_csv(train_file, delimiter='\\t', encoding='utf-8', header=0,\n",
    "                        names=['text', 'label'])\n",
    "\n",
    "    # to shuffle:\n",
    "    #train.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "    test = pd.read_csv(test_file, delimiter='\\t', encoding='utf-8', header=0,\n",
    "                        names=['text', 'label'])\n",
    "\n",
    "    print('vectorizing...')\n",
    "    vect = CountVectorizer()\n",
    "    classifier = NBSVM()\n",
    "\n",
    "    # create pipeline\n",
    "    clf = Pipeline([('vect', vect), ('nbsvm', classifier)])\n",
    "    params = {\n",
    "        'vect__token_pattern': r\"\\S+\",\n",
    "        'vect__ngram_range': ngram, \n",
    "        'vect__binary': True\n",
    "    }\n",
    "    clf.set_params(**params)\n",
    "\n",
    "    #X_train = vect.fit_transform(train['text'])\n",
    "    #X_test = vect.transform(test['text'])\n",
    "\n",
    "    print('fitting...')\n",
    "    clf.fit(train['text'], train['label'])\n",
    "\n",
    "    print('classifying...')\n",
    "    pred = clf.predict(test['text'])\n",
    "   \n",
    "    print('testing...')\n",
    "    acc = accuracy_score(test['label'], pred)\n",
    "    f1 = semeval_senti_f1(pred, test['label'])\n",
    "    print('NBSVM: acc=%f, f1=%f' % (acc, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def TryAroundModel_LG(X_train, X_test, Y_train, Y_test):\n",
    "    logistic_model = LogisticRegression(solver='lbfgs')\n",
    "    logistic_model.fit(X_train, Y_train)\n",
    "    LG_accuracy = accuracy_score(logistic_model.predict(X_test), Y_test)\n",
    "    print(\"Logistic Regression -- Accuracy: \", LG_accuracy)\n",
    "    \n",
    "    return (\"Logistic Regression\", LG_accuracy)\n",
    "\n",
    "# TryAroundModel_LG(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TryAroundModel_NB(X_train, X_test, Y_train, Y_test):\n",
    "    NB_model = MultinomialNB(alpha=7.4)\n",
    "    NB_model.fit(X_train, Y_train)\n",
    "    NB_accuracy = accuracy_score(NB_model.predict(X_test), Y_test)\n",
    "    \n",
    "    print(\"Multinomial Naive Bayes -- Accuracy: \", NB_accuracy)\n",
    "\n",
    "    return (\"Multinomial Naive Bayes\", NB_accuracy)\n",
    "\n",
    "# TryAroundModel_NB(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TryAroundModel_RF(X_train, X_test, Y_train, Y_test):\n",
    "    RF_model = RandomForestClassifier(n_estimators= 32)\n",
    "    RF_model = RF_model.fit(X_train, Y_train)\n",
    "    RF_accuracy = accuracy_score(RF_model.predict(X_test), Y_test)\n",
    "    \n",
    "    print(\"Random Forest -- Accuracy: \", RF_accuracy)\n",
    "\n",
    "    return (\"Random Forest\", RF_accuracy)\n",
    "\n",
    "# TryAroundModel_RF(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TryAroundModel_GBM(X_train, X_test, Y_train, Y_test):\n",
    "    GBM_model = GradientBoostingClassifier(n_estimators= 10, n_iter_no_change = 3)\n",
    "    GBM_model = GBM_model.fit(X_train, Y_train)\n",
    "    GBM_accuracy = accuracy_score(GBM_model.predict(X_test), Y_test)\n",
    "    \n",
    "    print(\"Gradient Boosting Machine -- Accuracy: \", GBM_accuracy)\n",
    "    \n",
    "    return (\"Gradient Boosting Machine\", GBM_accuracy)\n",
    "\n",
    "# TryAroundModel_GBM(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TryAroundModel_NBSVM(X_train, X_test, Y_train, Y_test):\n",
    "    NB_SVM_model = NBSVM()\n",
    "    NB_SVM_model = NB_SVM_model.fit(X_train, Y_train)\n",
    "    NB_SVM_accuracy = accuracy_score(NB_SVM_model.predict(X_test), Y_test)\n",
    "    \n",
    "    print(\"Naive Bayes SVM -- Accuracy: \", NB_SVM_accuracy)\n",
    "    \n",
    "    return (\"Naive Bayes SVM\", NB_SVM_accuracy)\n",
    "\n",
    "# TryAroundModel_NBSVM(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TryAroundModel_MPLNN(X_train, X_test, Y_train, Y_test):\n",
    "    \n",
    "    MPLNN_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape = (X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(2800, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0),\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0),\n",
    "        tf.keras.layers.Dense(2, activation = \"softmax\")\n",
    "    ])\n",
    "\n",
    "    MPLNN_model.compile(optimizer = 'rmsprop', loss = 'sparse_categorical_crossentropy')\n",
    "    MPLNN_model.fit(X_train, Y_train)\n",
    "    y_predict = MPLNN_model.predict_classes(X_test)\n",
    "    MPLNN_accuracy = accuracy_score(y_predict, Y_test)\n",
    "    \n",
    "    print(\"Multilayer Perceptron Neural Network(MLP) -- Accuracy: \", MPLNN_accuracy)\n",
    "    \n",
    "    return (\"Multilayer Perceptron Neural Network(MLP)\", MPLNN_accuracy)\n",
    "\n",
    "# TryAroundModel_MPLNN(X_train, X_test, Y_train, Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_data.Label.values\n",
    "Y_test = test_data.Label.values\n",
    "\n",
    "def TryAroundModel_LSTM(X_raw_text_train, X_raw_text_test, Y_train, Y_test):\n",
    "    max_features = 10000\n",
    "    max_len = 200\n",
    "    tokenizer = Tokenizer(nb_words = max_features)\n",
    "    tokenizer.fit_on_texts(X_raw_text_train)\n",
    "    sequences_train = tokenizer.texts_to_sequences(X_raw_text_train)\n",
    "    sequences_test = tokenizer.texts_to_sequences(X_raw_text_test)\n",
    "\n",
    "    X_train = sequence.pad_sequences(sequences_train, maxlen=max_len)\n",
    "    X_test = sequence.pad_sequences(sequences_test, maxlen=max_len)\n",
    "\n",
    "    Y_train = tf.keras.utils.to_categorical(Y_train)\n",
    "    Y_test = tf.keras.utils.to_categorical(Y_test)\n",
    "\n",
    "\n",
    "    LSTM_model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Embedding(input_dim = max_features, output_dim = 200, mask_zero = True),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.LSTM(300,\n",
    "                                 dropout= 0.2, recurrent_dropout=0.2),\n",
    "            tf.keras.layers.Dense(2, activation = \"softmax\", input_shape = (32,300))\n",
    "    ])\n",
    "\n",
    "\n",
    "    LSTM_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',metrics=['categorical_accuracy'])\n",
    "\n",
    "    \n",
    "    LSTM_model.fit(X_train, Y_train, batch_size=32, epochs=1)\n",
    "    score, acc = LSTM_model.evaluate(X_test, Y_test,\n",
    "                                batch_size=32)\n",
    "    \n",
    "    print(\"LSTM Neural Network -- Accuracy: \", acc)\n",
    "    \n",
    "    return (\"LSTM Neural Network\", acc)\n",
    "    \n",
    "    \n",
    "# TryAroundModel_LSTM(X_raw_text_train, X_raw_text_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_data.Label.values\n",
    "Y_test = test_data.Label.values\n",
    "\n",
    "def TryAroundModel_FB_LSTM(X_raw_text_train, X_raw_text_test, Y_train, Y_test):\n",
    "    max_features = 10000\n",
    "    max_len = 200\n",
    "    tokenizer = Tokenizer(nb_words = max_features)\n",
    "    tokenizer.fit_on_texts(X_raw_text_train)\n",
    "    sequences_train = tokenizer.texts_to_sequences(X_raw_text_train)\n",
    "    sequences_test = tokenizer.texts_to_sequences(X_raw_text_test)\n",
    "\n",
    "    X_train = sequence.pad_sequences(sequences_train, maxlen=max_len)\n",
    "    X_test = sequence.pad_sequences(sequences_test, maxlen=max_len)\n",
    "\n",
    "    Y_train = tf.keras.utils.to_categorical(Y_train)\n",
    "    Y_test = tf.keras.utils.to_categorical(Y_test)\n",
    "\n",
    "\n",
    "    forward_LSTM = tf.keras.layers.LSTM(300, dropout= 0.2, recurrent_dropout=0.2, return_sequences=True)\n",
    "    backward_LSTM = tf.keras.layers.LSTM(300, dropout= 0.2, recurrent_dropout=0.2, return_sequences=True, go_backwards = True)\n",
    "\n",
    "\n",
    "    LSTM_model = tf.keras.models.Sequential()\n",
    "    LSTM_model.add(tf.keras.layers.Embedding(input_dim = max_features, output_dim = 200, mask_zero = True))\n",
    "    LSTM_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(300, return_sequences=True), input_shape=(32, 200)))\n",
    "    LSTM_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(300)))\n",
    "    LSTM_model.add(tf.keras.layers.Dense(2, activation = \"softmax\"))\n",
    "\n",
    "\n",
    "    LSTM_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "    LSTM_model.fit(X_train, Y_train, batch_size=32, epochs=1)\n",
    "    \n",
    "    \n",
    "    score, acc = LSTM_model.evaluate(X_test, Y_test,\n",
    "                                batch_size=32)\n",
    "    \n",
    "    \n",
    "    print(\"Forward and Backward LSTM Neural Netword -- Accuracy: \", acc)\n",
    "    \n",
    "    return (\"Forward and Backward LSTM Neural Netword\", acc)\n",
    "    \n",
    "    \n",
    "# TryAroundModel_FB_LSTM(X_raw_text_train, X_raw_text_test, Y_train, Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw_text_train = joint_headlines_train\n",
    "X_raw_text_test = joint_headlines_test\n",
    "\n",
    "Y_train = train_data.Label.values\n",
    "Y_test = test_data.Label.values\n",
    "\n",
    "def TryAroundModel_CNN(X_raw_text_train, X_raw_text_test, Y_train, Y_test):\n",
    "    max_features = 10000\n",
    "    max_len = 200\n",
    "    tokenizer = Tokenizer(nb_words = max_features)\n",
    "    tokenizer.fit_on_texts(X_raw_text_train)\n",
    "    sequences_train = tokenizer.texts_to_sequences(X_raw_text_train)\n",
    "    sequences_test = tokenizer.texts_to_sequences(X_raw_text_test)\n",
    "\n",
    "    X_train = sequence.pad_sequences(sequences_train, maxlen=max_len)\n",
    "    X_test = sequence.pad_sequences(sequences_test, maxlen=max_len)\n",
    "\n",
    "    Y_train = tf.keras.utils.to_categorical(Y_train)\n",
    "    Y_test = tf.keras.utils.to_categorical(Y_test)\n",
    "\n",
    "    num_filters = 168\n",
    "    CNN_model = tf.keras.models.Sequential()\n",
    "    CNN_model.add(tf.keras.layers.Embedding(input_dim = max_features, output_dim = 200))\n",
    "    CNN_model.add(tf.keras.layers.Convolution1D(filters = num_filters, kernel_size = 20, activation = 'relu'))\n",
    "    \n",
    "    def Max1D(X):\n",
    "        return K.max(X, axis = 1)\n",
    "    \n",
    "    CNN_model.add(tf.keras.layers.Lambda(Max1D, output_shape = (num_filters, )))\n",
    "    CNN_model.add(tf.keras.layers.Dense(120, activation = 'relu'))\n",
    "    CNN_model.add(tf.keras.layers.Dropout(0.2))\n",
    "    CNN_model.add(tf.keras.layers.Dense(2, activation = 'softmax'))\n",
    "\n",
    "    CNN_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',metrics=['categorical_accuracy'])\n",
    "    \n",
    "    CNN_model.fit(X_train, Y_train, batch_size=32, epochs=1)\n",
    "    \n",
    "    score, acc = CNN_model.evaluate(X_test, Y_test,\n",
    "                            batch_size=32)\n",
    "    \n",
    "    print(\"Convolutional Neural Network -- Accuracy: \", acc)\n",
    "    \n",
    "    \n",
    "    return (\"Convolutional Neural Network\", acc)\n",
    "    \n",
    "    \n",
    "# TryAroundModel_CNN(X_raw_text_train, X_raw_text_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression -- Accuracy:  0.5567282321899736\n",
      "Multinomial Naive Bayes -- Accuracy:  0.5145118733509235\n",
      "Naive Bayes SVM -- Accuracy:  0.525065963060686\n",
      "Random Forest -- Accuracy:  0.5382585751978892\n",
      "Gradient Boosting Machine -- Accuracy:  0.49868073878627966\n",
      "1610/1610 [==============================] - 4s 3ms/sample - loss: 0.6986\n",
      "Multilayer Perceptron Neural Network(MLP) -- Accuracy:  0.5620052770448549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1610/1610 [==============================] - 7s 5ms/sample - loss: 0.6929 - categorical_accuracy: 0.5193\n",
      "379/379 [==============================] - 1s 3ms/sample - loss: 0.7068 - categorical_accuracy: 0.5066\n",
      "Convolutional Neural Network -- Accuracy:  0.5065963\n",
      "1610/1610 [==============================] - 38s 23ms/sample - loss: 0.6918 - categorical_accuracy: 0.5304\n",
      "379/379 [==============================] - 2s 6ms/sample - loss: 0.6952 - categorical_accuracy: 0.5066\n",
      "LSTM Neural Network -- Accuracy:  0.5065963\n",
      "1610/1610 [==============================] - 189s 118ms/sample - loss: 0.6923 - categorical_accuracy: 0.5311\n",
      "379/379 [==============================] - 8s 21ms/sample - loss: 0.6925 - categorical_accuracy: 0.5066\n",
      "Forward and Backward LSTM Neural Netword -- Accuracy:  0.5065963\n"
     ]
    }
   ],
   "source": [
    "def TryAroundModel(X_train, X_test, Y_train, Y_test, X_raw_text_train = None, X_raw_text_test = None):\n",
    "    accuracy_list = []\n",
    "    processed_arg = [X_train, X_test, Y_train, Y_test]\n",
    "    \n",
    "    \n",
    "    accuracy_list.append(TryAroundModel_LG(*processed_arg))\n",
    "    accuracy_list.append(TryAroundModel_NB(*processed_arg))\n",
    "    accuracy_list.append(TryAroundModel_NBSVM(*processed_arg))\n",
    "    accuracy_list.append(TryAroundModel_RF(*processed_arg))\n",
    "    accuracy_list.append(TryAroundModel_GBM(*processed_arg))\n",
    "    accuracy_list.append(TryAroundModel_MPLNN(*processed_arg))\n",
    "    \n",
    "    if len(X_raw_text_train) != 0 and len(X_raw_text_test) != 0:\n",
    "        raw_arg = [X_raw_text_train, X_raw_text_test, Y_train, Y_test]\n",
    "    \n",
    "        accuracy_list.append(TryAroundModel_CNN(*raw_arg))\n",
    "        accuracy_list.append(TryAroundModel_LSTM(*raw_arg))\n",
    "        accuracy_list.append(TryAroundModel_FB_LSTM(*raw_arg))\n",
    "        \n",
    "    return accuracy_list\n",
    "\n",
    "accuracy_list = TryAroundModel(X_train, X_test, Y_train, Y_test, X_raw_text_train, X_raw_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multilayer Perceptron Neural Network(MLP)</td>\n",
       "      <td>0.562005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.556728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.538259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes SVM</td>\n",
       "      <td>0.525066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.514512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "      <td>0.506596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSTM Neural Network</td>\n",
       "      <td>0.506596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Forward and Backward LSTM Neural Netword</td>\n",
       "      <td>0.506596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gradient Boosting Machine</td>\n",
       "      <td>0.498681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           0         1\n",
       "0  Multilayer Perceptron Neural Network(MLP)  0.562005\n",
       "1                        Logistic Regression  0.556728\n",
       "2                              Random Forest  0.538259\n",
       "3                            Naive Bayes SVM  0.525066\n",
       "4                    Multinomial Naive Bayes  0.514512\n",
       "5               Convolutional Neural Network  0.506596\n",
       "6                        LSTM Neural Network  0.506596\n",
       "7   Forward and Backward LSTM Neural Netword  0.506596\n",
       "8                  Gradient Boosting Machine  0.498681"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sorted(accuracy_list, key = lambda kv: kv[1], reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
